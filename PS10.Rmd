---
title: 'STAT/MATH 495: Problem Set 10'
author: "Leonard Yoon"
date: '2017-11-28'
output:
  html_document:
    collapsed: no
    df_print: kable
    smooth_scroll: no
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=8, fig.height=4.5, message=FALSE, warning = FALSE)
```

# Collaboration

Please indicate who you collaborated with on this assignment: ANDREW (Big help!)

# Setup

```{r, include = FALSE}
library(tidyverse)
library(broom)
library(glmnet)
library(MLmetrics)

train <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")

# Only use 150 observations to train model!
set.seed(76)
train <- train %>% 
  mutate(log_price_doc = log(price_doc)) %>% 
  sample_n(150)

# Need "dummy" outcome variable to make model.matrix() code below work
test <- test %>% 
  mutate(log_price_doc=1) 

# Model formula
model_formula <- as.formula("log_price_doc ~ full_sq + area_m + raion_popul + green_zone_part + indust_part + children_preschool + preschool_education_centers_raion + children_school + school_education_centers_raion + school_education_centers_top_20_raion + healthcare_centers_raion + university_top_20_raion + sport_objects_raion + additional_education_raion + culture_objects_top_25 + culture_objects_top_25_raion + shopping_centers_raion + office_raion + thermal_power_plant_raion + incineration_raion + oil_chemistry_raion + radiation_raion + railroad_terminal_raion + big_market_raion + nuclear_reactor_raion + detention_facility_raion + full_all + male_f + female_f + young_all + young_male + young_female + work_all + work_male + work_female + ekder_all + ekder_male + ekder_female + ID_metro + metro_min_avto + metro_km_avto + kindergarten_km + school_km + park_km + green_zone_km + industrial_km + water_treatment_km + cemetery_km + incineration_km + railroad_station_avto_km + railroad_station_avto_min + ID_railroad_station_avto + public_transport_station_km + public_transport_station_min_walk + water_km + water_1line + mkad_km + ttk_km + sadovoe_km + bulvar_ring_km + kremlin_km + big_road1_km + ID_big_road1 + big_road1_1line + big_road2_km + ID_big_road2 + railroad_km + railroad_1line + zd_vokzaly_avto_km + ID_railroad_terminal + bus_terminal_avto_km + ID_bus_terminal + oil_chemistry_km + nuclear_reactor_km + radiation_km + power_transmission_line_km + thermal_power_plant_km + ts_km + big_market_km + market_shop_km + fitness_km + swim_pool_km + ice_rink_km + stadium_km + basketball_km + hospice_morgue_km + detention_facility_km + public_healthcare_km + university_km + workplaces_km + shopping_centers_km + office_km + additional_education_km + preschool_km + big_church_km + church_synagogue_km + mosque_km + theater_km + museum_km + exhibition_km + catering_km + green_part_500 + prom_part_500 + office_count_500 + office_sqm_500 + trc_count_500 + trc_sqm_500") 

# Define predictor matrices
predictor_matrix_train <- model.matrix(model_formula, data = train)[, -1]
predictor_matrix_test <- model.matrix(model_formula, data = test)[, -1]

# Define values of tuning/complexity parameter lambda. Note:
# -They increase at an exponential rate in powers of 10
lambda_inputs <- 10^seq(-2, 10, length = 100)
```

# Do work and create submission files:

## LASSO crossvalidated

```{r}
# Crossvalidated LASSO for training data
LASSO_CV_train <- cv.glmnet(x=predictor_matrix_train, y=train$log_price_doc, alpha = 1, lambda = lambda_inputs)

# Optimal lambdas for training data
lambda_optimal_train <- LASSO_CV_train$lambda.min # Value of lambda corresponding to the minimal MSE
lambda_optimal_one_SE_train <- LASSO_CV_train$lambda.1se # Value of lambda corresponding to the simplest
# model within one standard error of minimal MSE
```

Let's use the value of $\lambda$ corresponding to the simplest model within one standard error of minimal MSE for our predictions.

```{r}
# predict log_price_doc for training data
y_hat_train <- predict(LASSO_CV_train, newx=predictor_matrix_train, s=lambda_optimal_one_SE_train) %>% 
  as.vector()

# revert units of predicted log_price_doc to units of price_doc
y_hat_train <- exp(y_hat_train)

# root mean squared log error
train_LASSO_score <- RMSLE(y_hat_train, train$price_doc)
```

```{r}
# predict log_price_doc for test data
y_hat_test <- predict(LASSO_CV_train, newx=predictor_matrix_test, s=lambda_optimal_one_SE_train) %>% 
  as.vector()

# revert units of predicted log_price_doc to units of price_doc
y_hat_test <- exp(y_hat_test)

# create submission.csv
submission <- data.frame(id = test$id, 
                         price_doc = y_hat_test)
write.csv(submission, "submission.csv", row.names = FALSE)
```

## lm

```{r}
# lm for training data
linear_model_train <- lm(model_formula, data=train)

# predict log_price_doc
y_hat_train_lm <- predict(linear_model_train)

# revert units of predicted log_price_doc to units of price_doc
y_hat_train_lm <- exp(y_hat_train_lm)
train_lm_score <- RMSLE(y_hat_train_lm, train$price_doc)
```

```{r}
# lm for test data
linear_model_test <- lm(model_formula, data=test)

# predict log_price_doc
y_hat_test_lm <- predict(linear_model_test)

# revert units of predicted log_price_doc to units of price_doc
y_hat_test_lm <- exp(y_hat_test_lm)
```

```{r}
# create submission_lm.csv
submission_lm <- data.frame(id = test$id, 
                         price_doc = y_hat_test_lm)
write.csv(submission_lm, "submission_lm.csv", row.names = FALSE)
```

# Scoreboard

Using the "scoring mechanism" for the Russian Housing Kaggle competition, fill
in these cells:


Method                | Training Score  | Kaggle Score
--------------------- | -------------   | -------------
lm                    |  `r train_lm_score`               |  14.4037
LASSO crossvalidated  |  `r train_LASSO_score`               |    .42577

LASSO crossvalidation works! And we never want to overfit to training data. Gotta fit to test data to determine how good is our model.